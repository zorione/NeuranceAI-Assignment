{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Packages","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"### Basic Packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Ref: https://docs.python.org/3/library/string.html\nimport re,string,unicodedata\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl\n\n### NLTK Imports\nimport nltk\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import sentiwordnet as swn, wordnet\nfrom nltk.corpus.reader.wordnet import WordNetError\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ref: https://www.nltk.org/data.html\n# Ref: https://www.nltk.org/_modules/nltk/corpus.html\nnltk.download('omw-1.4')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntf.random.set_seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploration","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/neuranceai/train.csv\")\ndf_test  = pd.read_csv(\"../input/neuranceai/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.info())\ndf_train.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.info())\ndf_test.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TRAINING DATASET\")\nprint(\"#Unique Drug Names: \", len(np.unique(df_train['name_of_drug'])))\nprint(\"#Unique Use Cases: \", len(np.unique(df_train['use_case_for_drug'])))\n\nprint(\"\\nTEST DATASET\")\nprint(\"#Unique Drug Names: \", len(np.unique(df_test['name_of_drug'])))\nprint(\"#Unique Use Cases: \", len(np.unique(df_test['use_case_for_drug'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Pre-Processing","metadata":{}},{"cell_type":"code","source":"# Making a list of all the stopwords\nstop_words = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop_words.update(punctuation)\n\n# A function to determine the tag for every word\n# Ref: https://www.nltk.org/api/nltk.tag.html\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n    \n# Creating a function to lemmatize the review text\n# Ref: https://www.nltk.org/_modules/nltk/stem/wordnet.html\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(review_by_patient):\n    final_text = []\n    for i in review_by_patient.split():\n        if i.strip().lower() not in stop_words:\n            # Tag of the word, used for lemmatization\n            pos = pos_tag([i.strip()]) \n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return \" \".join(final_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_df_train = os.path.isfile(\"../input/neuranceai/new_df_train.csv\")\nis_df_test  = os.path.isfile(\"../input/neuranceai/new_df_test.csv\")\n\nif is_df_train and is_df_test:\n    new_df_train = pd.read_csv(\"../input/neuranceai/new_df_train.csv\")\n    new_df_test = pd.read_csv(\"../input/neuranceai/new_df_test.csv\")\nelse:\n    reviews_train = df_train['review_by_patient']\n    reviews_test = df_test['review_by_patient']\n    print(reviews_train.shape, reviews_test.shape)\n\n    # Performing Lemmatization\n    reviews_train = reviews_train.apply(lemmatize_words)\n    reviews_test = reviews_test.apply(lemmatize_words)\n    print(reviews_train.shape, reviews_test.shape)\n\n    # Creating a new dataset with lemmatized words\n    new_df_train = df_train.drop(['review_by_patient'], axis = 1)\n    new_df_test  = df_test.drop(['review_by_patient'], axis = 1)\n    print(new_df_train.shape, new_df_test.shape)\n\n    new_df_train = pd.concat([new_df_train, reviews_train], axis = 1)\n    new_df_test = pd.concat([new_df_test, reviews_test], axis = 1)\n    print(new_df_train.shape, new_df_test.shape)\n\n    new_df_train.to_csv(\"new_df_train.csv\", index = False)\n    new_df_test.to_csv(\"new_df_test.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing the variables from the memory, only works with one variable at a time\n# reset_selective -f <variable>\n\n# To find the variables in the memory\n# who_ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_indices_train = os.path.isfile(\"../input/neuranceai/indices_review_train_100.csv\")\nis_indices_test  = os.path.isfile(\"../input/neuranceai/indices_review_test_100.csv\")\n\nif is_indices_train and is_indices_test:\n    indices_tr = pd.read_csv(\"../input/neuranceai/indices_review_train_100.csv\")\n    indices_te = pd.read_csv(\"../input/neuranceai/indices_review_test_100.csv\")\nelse:\n    reviews_train = new_df_train['review_by_patient']\n    reviews_test = new_df_test['review_by_patient']\n\n    count_vec = CountVectorizer(max_features = 5000)\n    count_vec.fit(reviews_train)\n    vocab = count_vec.vocabulary_\n    features = count_vec.get_feature_names_out()\n    analyzer = count_vec.build_analyzer()\n    \n    def sentence_to_indices(sentence, max_len = 50):\n        indices = []\n        words = analyzer(sentence)\n        for word in words:\n            if word in features:\n                indices.append(vocab[word])\n        while len(indices) < max_len:\n            indices.append(-1)\n        return indices[:max_len]\n\n    min_index = min(vocab.values())\n    max_index = max(vocab.values())\n    print(min_index, max_index)\n\n    indices_train =  []\n    for rev in tqdm(reviews_train):\n        indices = sentence_to_indices(rev, 50)\n        indices_train.append(indices)\n\n    indices_test = []\n    for rev in tqdm(reviews_test):\n        indices = sentence_to_indices(rev, 50)\n        indices_test.append(indices)\n\n    indices_tr = pd.DataFrame(indices_train)\n    indices_tr.to_csv(\"indices_review_train.csv\", index = False)\n    indices_te = pd.DataFrame(indices_test)\n    indices_te.to_csv(\"indices_review_test.csv\", index = False)    \n    \nprint(indices_tr.shape, indices_te.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preparing the Dataset for modelling purposes","metadata":{}},{"cell_type":"code","source":"X_train = new_df_train.drop(['patient_id', 'name_of_drug', 'use_case_for_drug', \n    'drug_approved_by_UIC', 'review_by_patient', 'base_score'], axis = 1)\nY_train = new_df_train['base_score']\nX_test = new_df_test.drop(['patient_id', 'name_of_drug', 'use_case_for_drug', \n    'drug_approved_by_UIC', 'review_by_patient'], axis = 1)\ntest_ids = new_df_test['patient_id']\nprint(\"Training Set:\", X_train.shape, Y_train.shape)\nprint(\"Test Set:\", X_test.shape, test_ids.shape)\n\nX_train = pd.concat([X_train, indices_tr], axis = 1)\nX_test = pd.concat([X_test, indices_te], axis = 1)\nprint(\"Training Set:\", X_train.shape, Y_train.shape)\nprint(\"Test Set:\", X_test.shape, test_ids.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dividing the labelled examples into training and validation examples\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)\nprint(x_train.shape, x_val.shape, y_train.shape, y_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Clearing the memory","metadata":{}},{"cell_type":"code","source":"reset_selective -f df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_selective -f df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_selective -f X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_selective -f Y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_selective -f indices_tr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_selective -f indices_te","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Training the Model","metadata":{}},{"cell_type":"code","source":"# Training a model based on the 2 numerical features\nmodel1 = LinearRegression()\nmodel1.fit(x_train.iloc[:,:2], y_train)\n\npreds_train = model1.predict(x_train.iloc[:,:2])\npreds_val   = model1.predict(x_val.iloc[:,:2])\n\nRMSE_train = MSE(y_train, preds_train, squared = False)\nRMSE_val   = MSE(y_val, preds_val, squared = False)\n\nprint(\"Root Mean Squared Error for Training Set:\", RMSE_train)\nprint(\"Root Mean Squared Error for Validation Set:\", RMSE_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot Encoding\ndef ohe(x, depth):\n    # x_new = x[..., np.newaxis]\n    x_new = np.zeros((x.shape[0], x.shape[1], depth))\n    for i in range(len(x)):\n        for j in range(len(x[i])):\n            x_new[i][j][x[i][j]] = 1\n    return x_new\n\nx_train_oh = ohe(np.array(x_train.iloc[:,2:]), 5000)\nx_val_oh   = ohe(np.array(x_val.iloc[:,2:]), 5000)\nprint(x_train_oh.shape, x_val_oh.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = tf.keras.Sequential([\n    tfl.Masking(mask_value = -1),\n    tfl.LSTM(units = 64, activation = 'linear', return_sequences = False),\n    tfl.Dense(units = 1, activation = 'linear')\n])\n\nmodel2.compile(optimizer='sgd', loss='mae')\nmodel2.fit(x_train_oh, y_train, batch_size = 32, epochs = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_train = model2.predict(x_train_oh)\npreds_val   = model2.predict(x_val_oh)\n\nRMSE_train = MSE(y_train, preds_train, squared = False)\nRMSE_val   = MSE(y_val, preds_val, squared = False)\n\nprint(\"Root Mean Squared Error for Training Set:\", RMSE_train)\nprint(\"Root Mean Squared Error for Validation Set:\", RMSE_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now, here based on the performance of the 2 models, we can take the weighted sum of predictions, of the 2 models, and use the weighted sum as the final predictions.","metadata":{}}]}